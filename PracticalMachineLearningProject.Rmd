---
title: "Coursera Practical Machine Learning Project"
author: "Mark S."
date: "Wednesday, July 22, 2015"
output: html_document
---

## Project Summary
The goal of this project is to build a model that can predict the manner in which an exercise is being performed. We are given two sets of data, a training set and a test set. The source and description of the data is found here: http://groupware.les.inf.puc-rio.br/har. The data can be downloaded via the following links:

Training: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

Testing: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

## Data Overview and Strategy
My interpretation of the problem is as follows: "Given a set of measurements *in this moment*, predict how the exercise is being performed by any person *in this moment*."

The "*in this moment*" phrase is emphasized to establish the fact that we want a model that can generalize to any moment in time. We don't want to make a prediction based off of measurements that occured in the past; rather we would like to make predictions based on single measurements occuring at a single moment in time. Also, we would like a model that can generalize to any user. With this reasoning, we will ignore any variables provided that are considered "window statistics", variables involving time, and the names of the participants. This includes the first 7 variables of the data set, as well as any variable name containing min, max, avg, var, std, total, kurtosis, skewness, or amplitude.

Since this is a classification task, we'll use a random forest model. The entire process will be structured as follows:

1. Remove unwanted variables (explained above)
2. Split training data into 2 sets; call them "A" and "B" with a 70/30 split.
3. Perform data analysis and preprocessing using data set "A"
4. Perform feature selection using cross validation.
5. Build a random forest with selected features; evaluate its out of sample error with the hold-out set "B"
6. Make a prediction with the "final test" set; the data we must predict for submission.


## Downloading and Cleaning the Data
First we download the data, and remove the unwanted columns, as explained previously:

```{r, echo=FALSE}
suppressWarnings(suppressMessages(library(dplyr)))
suppressWarnings(suppressMessages(library(caret)))
suppressWarnings(suppressMessages(library(ggplot2)))
suppressWarnings(suppressMessages(library(RCurl)))


```

```{r}
#Set global seed for reproducibility
set.seed(8080)

#Load the data; use library RCurl to allow download of data in knitr
#note that some of the data contain "NA", "#DIV/0!" and empty values. These will be considered "NA"
temp<-getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",ssl.verifypeer=0L,followlocation=1L)
data<- tbl_df(read.csv(text=temp, na.strings = c("NA","#DIV/0!","")))

temp<-getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",ssl.verifypeer=0L,followlocation=1L)
final.test<- tbl_df(read.csv(text=temp, na.strings = c("NA","#DIV/0!","")))

#remove temp variable
rm(temp)

#First 7 columns aren't measurements; they won't be included
data<- data[,-(1:7)]
final.test<- final.test[,-(1:7)]

#Create a vector of all feature names based on time or window statistics
feats<- grep("^min|max|avg|var|std|total|kurtosis|skewness|amplitude",names(data))

#Remove these features from the data
data<- data[,-feats]
final.test<-final.test[,-feats]
```

Now we split the data into train and test sets with a 70/30 split.  The "final.test" set is the one we will make predictions on and submit.

```{r}
#split data into train/test sets
n<- createDataPartition(y=data$classe,p=0.7,list=FALSE)
train<- data[n,]
test<- data[-n,]

#convert data to tbl_df to use with dplyr
train<- tbl_df(train)
test<- tbl_df(test)

```

## Exploratory Data Analysis
First check the dimensionality of the data, and get an idea of the distribution of the classe variable:

```{r}
dim(train)
table(train$classe)
```
The classe variable isn't perfectly balanced, but it shouldn't be enough to cause any problems.

We've already eliminated many of the features (reducing from 159 to 48). We should also check to see if we have any features with near-zero variance:
```{r}
nearZeroVar(train)
```

It would be difficult to plot feature pairs, but we can visualze the correlations to identify variables to inspect:
```{r,echo=FALSE, fig.width=10,fig.height=8}
# Correlation plot, looking for correlation between variables
library(lattice)
library("RColorBrewer")

# "PRGn" and "Spectral" work well
brewer.div = colorRampPalette(brewer.pal(11, "PRGn"), interpolate = "spline")

levelplot(cor(train[,-49]),at=do.breaks(c(-1.01,1.01),20),
          main="Correlation Level Plot", xlab="",ylab="",aspect=1,
          #panel=panel.corrgram.2,
          colorkey=list(space="top"),pretty=TRUE, 
          #col.regions=colorRampPalette(c("red","white","blue")),
          col.regions=brewer.div(200),
          scales=list(x=list(rot=90)))
```

Based on the correlation plot it seems that only a handful of variables are highly correlated. We'll build a random forest using 10-fold cross validation to determine the most important features to use in the final model:

```{r,warning=FALSE,message=FALSE}
fitControl<- trainControl(method="cv",number=10)
mod0<- train(classe~.,data=train, trControl= fitControl, method="rf", ntree = 20, allowParallel=TRUE)
```

We can plot the important features to see which ones we should keep in our final model:

```{r}
plot(varImp(mod0))
```

It appears we could get away using the top 10 to 15 features. We'll err on the safe side and keep the top 15 variables for the final model. We'll extract these features and prepare the data for the final model:

```{r}
#Feature Importances
names <- rownames(varImp(mod0)$importance)
importance <- varImp(mod0)$importance$Overall
imps<- tbl_df(data.frame(names,importance))

#Custom function to Pull out column as vector from dplyr data frame
#Usage: pull(data,"column_name") or pull(data, column_name) or pull(data,column_number)
pull <- function(x,y) {x[,if(is.name(substitute(y))) deparse(substitute(y)) else y, drop = FALSE][[1]]}

#Extract important feature names, use it to only keep important variables in data
imp.feats<- as.character(pull(arrange(imps,-importance),names))[1:15]
imp.train<- train[,c(imp.feats,"classe")]
imp.test<- test[,c(imp.feats,"classe")]

```

Now we'll build our final random forest model.

## Building the Model
First, we build the model:

```{r,warning=FALSE,message=FALSE}

#Build the model; number of trees was chosen to reduce the time to build the model
mod1<- train(classe~., data = imp.train, method = "rf",ntree= 20, allowParallel=TRUE)
```

Now, test the model on the test set to get an estimate of the out-of-sample error:

```{r}
#Make predictions on the test set
pred1<- predict(mod1,imp.test[,-16])

#Print the confusion matrix for the model
confusionMatrix(pred1,imp.test$classe)
```

The has an out-of-sample error estimate of approximately 1.4%. This model will be used to predict the final test set:

```{r}
predict(mod1,final.test[,-16])
```
